{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19aacc3e",
   "metadata": {},
   "source": "# 👩‍🏫 *MEGAN: Multi Explanation Graph Attention Network*\n\nThe `Megan` model is a *graph neural network (GNN)* architecture designed to be *self-explainable* - meaning that it outputs not only the primary target property prediction but also an explanation of its own decision simultaneously. The model achieves this through a sophisticated multi-channel attention mechanism that identifies the most relevant nodes and edges in the input graph which provide evidence for or against the predicted property.\n\n## Why Multi-Channel Explanations?\n\nUnlike traditional GNNs that produce a single prediction, MEGAN employs multiple \"explanation channels\" - parallel processing pathways that each learn to focus on different aspects of the graph structure. For regression tasks, typically two channels are used: one identifying graph features that contribute positively to the prediction, and another identifying features that contribute negatively. This dual-channel approach provides more nuanced and interpretable explanations than single-attention mechanisms.\n\n## Self-Supervised Explanation Training\n\nThe key innovation in MEGAN is the *explanation co-training* approach. During training, the model not only learns to make accurate predictions but also learns to make those predictions explainable. This is achieved through a self-supervised explanation loss that forces each channel's attention weights to be predictive of the target property by themselves - without relying on the actual node features. This constraint ensures that the attention mechanisms capture genuinely important structural patterns rather than arbitrary correlations.\n\nThis notebook demonstrates the complete workflow: dataset preparation, model configuration, training with explanation co-training, and generating interpretable predictions with visual explanations."
  },
  {
   "cell_type": "markdown",
   "id": "99fc5005",
   "metadata": {},
   "source": [
    "# 🧪 Loading the Dataset\n",
    "\n",
    "As a first step, we can load the dataset that will be used to demonstrate the model capabilities. This example dataset contains roughtly 10k molecules which are annotated with the Crippen's logP values as target property. This value can be deterministically calculated by `rdkit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# --- loading the dataset ---\n",
    "\n",
    "# This is the path to the folder in which this notebook is located in.\n",
    "PATH: str = os.getcwd()\n",
    "# For the purpose of this example, we use the dataset file \"clogp.csv\" that is located \n",
    "# in the same \"examples\" folder as this notebook.\n",
    "DATASET_PATH: str = os.path.join(PATH, \"clogp.csv\")\n",
    "\n",
    "# The example dataset is a CSV file consisting of molecular data in the format of \n",
    "# SMILES strings along with their corresponding clogP annotations. We can use \n",
    "# pandas, for example, to load this dataset.\n",
    "dataset: pd.DataFrame = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('\\n dataset size:')\n",
    "print(len(dataset))\n",
    "print('\\n dataset preview:')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59977720",
   "metadata": {},
   "source": [
    "# ⚙️ Dataset Processing\n",
    "\n",
    "After loading the dataset in the previous step, the individual elements are still represented as SMILES strings. The `Megan` model, however, is a *graph neural network (GNN)* and thus requires a *graph representation* of these molecule. For this purpose we can use the `MoleculeProcessing` class from the `visual_graph_datasets` library, which converts the SMILES strings into appropriate graph representations.\n",
    "\n",
    "More specifically, the `MoleculeProcessing` class provides the `process(smiles: str) -> dict` method which converts the SMILES string into a dictionary representation of the graph. This dictionary contains various numpy arrayes representing different aspects of the graph such as list of the atom and bond types, the connectivity of the graph in the form of an edge index list and the already featurized node and edge feature arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "from visual_graph_datasets.processing.molecules import MoleculeProcessing\n",
    "\n",
    "# The SMILES representation of caffeine, which we want to use as an example for \n",
    "# the conversion into a graph representation.\n",
    "SMILES = 'C1=CC=C2C=C(CCN)C=CC2=C1'\n",
    "\n",
    "processing = MoleculeProcessing()\n",
    "graph: dict = processing.process(SMILES)\n",
    "\n",
    "print('\\n attributes of the graph dictionary:')\n",
    "pprint(list(graph.keys()))\n",
    "print('\\n graph dictionary:')\n",
    "pprint(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea03317",
   "metadata": {},
   "source": [
    "In addition to the conversion of a SMILES to a graph dictionary representation, the `MoleculeProcessing` class also provides the `visualize_as_figure(smiles: str) -> plt.Figure, ndarray` method which can be used to visualize the molecule as a 2D figure (using RDKit under the hood).\n",
    "\n",
    "Crucially, this method returns a tuple of two values, the first being the matplotlib figure object and the second being an array of `node_positions` containing the 2D coordinates of each atom of the molecule in the coordinate system of the figure. This information is essential later on for the visualization of the node importance explanations provided by the `Megan` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, node_positions = processing.visualize_as_figure(SMILES, width=1000, height=1000)\n",
    "\n",
    "# To illustrate the the node positions, we will plot the node index of each atom on top \n",
    "# of the molecular illustration.\n",
    "for node_idx, (x, y) in enumerate(node_positions):\n",
    "    ax = fig.axes[0]\n",
    "    ax.text(x, 1000-y, str(node_idx), fontsize=10, color='black')\n",
    "    \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947b9a4",
   "metadata": {},
   "source": [
    "# 🤖 Setting up the Model\n",
    "\n",
    "In the next step, we need to initialize the `Megan` model. This can be done by instantiating the `Megan` class. The constructor accpets a larger number of parameters, the most important ones for the core functionality of the model being briefly explained in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_attention_student import Megan\n",
    "\n",
    "model = Megan(\n",
    "    # --- prediction config ---\n",
    "    # These first parameters need to specify how many node and edge features are used \n",
    "    # for the graphs that will be input to the model. This information can be obtained \n",
    "    # from the MoleculeProcessing object that was used to create the graphs.\n",
    "    node_dim=processing.get_num_node_attributes(),\n",
    "    edge_dim=processing.get_num_edge_attributes(),\n",
    "    # This list determines the structure of the message passing layers in the encoder.\n",
    "    # In this case we use three message passing steps with 64 hidden units each.\n",
    "    units=[64, 64, 64], \n",
    "    # Likewise, this list determines the structure of the final prediction MLP. For \n",
    "    # this one, it is important that the last entry matches the number of target \n",
    "    # values to be predicted (i.e. 1 for regression, or the number of classes\n",
    "    # for classification).\n",
    "    final_units=[64, 32, 1],\n",
    "    # This flag indicates that we want to perform a regression task. For binary \n",
    "    # classification this would be 'bce' and for multi-class classification\n",
    "    # 'classification'.\n",
    "    prediction_mode='regression',\n",
    "    # The learning rate controls how large the steps are that are taken during\n",
    "    # gradient descent optimization.\n",
    "    learning_rate=1e-4,\n",
    "    # --- explanation config ---\n",
    "    # Following parameters determine the configuration of the explanation mechanism.\n",
    "    importance_mode='regression',\n",
    "    # Most importantly setting this importance factor to a non-zero value activates \n",
    "    # the explanation consistency co-training loss.\n",
    "    importance_factor=1.0,\n",
    "    # Likewise, this factor modulates the weight of the sparsity loss that is \n",
    "    # applied to the explanation masks during training.\n",
    "    sparsity_factor=0.5,\n",
    "    # This value also influences the sparsity of the resulting explanations. The higher \n",
    "    # this value, the more sparse the explanation will be (meaning that less nodes \n",
    "    # will be highlighted as important). Lower values will highlight more nodes.\n",
    "    importance_offset=1.0,\n",
    ")\n",
    "\n",
    "print('\\n model summary:')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746cfb8",
   "metadata": {},
   "source": [
    "# 🤖 Model Training\n",
    "\n",
    "The model is trained with `pytorch_lightning`, which provides a high-level interface for training PyTorch models. \n",
    "\n",
    "To load the training data, the `graph_attention_student` package provides the special `SmilesDataset` class which is a subclass of `torch.utils.data.IterableDataset` that provides an efficient lazy-loaded datastream from the given CSV file. The dataset class implements reservoir sampling to ensure proper shuffling of the data during training. This dataset class will then act as an iterator instance for PyG `Data` objects representing the individual molecular graphs in the dataset and can then be used to create a PyG `DataLoader` instance.\n",
    "\n",
    "Finally, the model can be trained by instantiating a `pytorch_lightning.Trainer` object and calling its `fit` method with the model and the training data loader as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba454d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch_geometric.loader import DataLoader\n",
    "from graph_attention_student import SmilesDataset\n",
    "\n",
    "# --- training preparation ---\n",
    "\n",
    "dataset = SmilesDataset(\n",
    "    dataset=DATASET_PATH,\n",
    "    smiles_column='smiles',\n",
    "    target_columns=['value'],\n",
    "    processing=processing,\n",
    "    # This will randomize the order of the streamed dataset - effectively \n",
    "    # acting as a shuffling mechanism.\n",
    "    reservoir_sampling=True,\n",
    ")\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=64, \n",
    "    #shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=150,\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cf41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- model training ---\n",
    "# This function will start the actual training of the model and may therefore take\n",
    "# a few minutes.\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=loader_train,\n",
    ")\n",
    "\n",
    "# Important: After the training is completed, we need to explicitly put the model \n",
    "# into evaluation mode to ensure that the BatchNorm layers work propertly during \n",
    "# inference.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0e9e2",
   "metadata": {},
   "source": [
    "# 💾 Saving and Loading the Model\n",
    "\n",
    "After training a model, one may want to save that model to the disk to continue training later or to use it for inference. The `Megan` class provides convenient methods for saving and loading the model. The model can be saved to a checkpoint file using the `save(path: str)` method and can be loaded back using the class method `load(path: str) -> Megan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(PATH, \"model.ckpt\")\n",
    "\n",
    "# --- model saving ---\n",
    "# The model can be easily saved into a persistent file representation using the \n",
    "# the \"save\" method.\n",
    "model.save(MODEL_PATH)\n",
    "\n",
    "# --- model loading ---\n",
    "# Equally, an object instance of the model can be recreated from the saved file \n",
    "# representation using the \"load\" method of the Megan class.\n",
    "model: Megan = Megan.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2845c",
   "metadata": {},
   "source": [
    "# 🔮 Model Prediction\n",
    "\n",
    "To query a prediction for an unkown molecule, the `Megan` model provides the `forward_graph(graph: dict) -> dict` method which accepts a graph dictionary representation of the molecule (as provided by the `MoleculeProcessing` class) and returns a dictionary containing the predicted property value as well as various other artifacts of the model forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bb8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "graph = processing.process(SMILES)\n",
    "\n",
    "results: dict = model.forward_graph(graph)\n",
    "print('\\n prediction results:')\n",
    "pprint(list(results.keys()))\n",
    "\n",
    "print('\\n predicted value:')\n",
    "pprint(results['graph_output'].item())\n",
    "\n",
    "print('\\n true value:')\n",
    "true_value = Chem.Crippen.MolLogP(Chem.MolFromSmiles(SMILES))\n",
    "pprint(true_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a571b",
   "metadata": {},
   "source": [
    "The previous cell shows how to obtain the main property prediction from the trained model. However, one main use case of the `Megan` model is the explaination of its own predictions. The function `megan_prediction_report` provides a convenient interface to visualize not only the prediction but the explanation as well. This function will create a report PDF that includes the visualization of the molecule, the predicted property value and a heatmap visualization of the node and edge importance scores as provided by the model's attention mechanism. To do this, the function needs to receive the SMILES string to be explained, the trained model as well as the `MoleculeProcessing` instance that was used to featurize the dataset. The report will be saved to the specified `output_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "from graph_attention_student.torch.advanced import megan_prediction_report\n",
    "\n",
    "output_path = os.path.join(PATH, \"megan_prediction_report.pdf\")\n",
    "megan_prediction_report(\n",
    "    value=SMILES,\n",
    "    model=model,\n",
    "    processing=processing,\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "IFrame(output_path, width=600, height=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}