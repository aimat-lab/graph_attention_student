"""
This example shows how to train a MEGAN model based on a chemistry dataset consisting of a CSV files
containing the SMILES representations of the molecules.

"""
import os
import csv
import random
from typing import List, Dict, Any

from pycomex.experiment import Experiment
from pycomex.util import Skippable

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as ks
from matplotlib.backends.backend_pdf import PdfPages
from kgcnn.data.moleculenet import OneHotEncoder
from imageio.v2 import imread

from graph_attention_student.util import DATASETS_FOLDER
from graph_attention_student.data import create_molecule_eye_tracking_dataset
from graph_attention_student.data import load_eye_tracking_dataset
from graph_attention_student.data import process_graph_dataset
from graph_attention_student.training import NoLoss
from graph_attention_student.training import LogProgressCallback
from graph_attention_student.models import MultiAttentionStudent
from graph_attention_student.visualization import plot_node_importances
from graph_attention_student.visualization import plot_edge_importances

# == DATASET PARAMETERS ==
# [!] Insert the absolute path to the CSV file here
CSV_PATH = os.path.join(DATASETS_FOLDER, 'dataset-E.csv')
# [!] Insert the column name of the smiles column here
SMILES_COLUMN = "SMILES"
# [!] Insert the column name of the target value here
TARGET_COLUMN = 'Solubility'
TRAIN_SPLIT = 0.8

# == MODEL PARAMETERS ==

# This list defines how many graph convolutional layers to configure the network with. Every element adds
# one layer. The numbers themselves are the hidden units to be used for those layers.
UNITS = [50, 40, 30]

# The dropout rate which is applied after EVERY graph convolutional layer of the network. Especially for
# large networks (>20 hidden units and multiple importance channels, this dropout proves very useful)
DROPOUT_RATE = 0.1

# This is the weight of the additional explanation-only step which is being applied to the network.
# This explanation only step is important to develop actually interpretable explanations. Refer to the
# paper for more details about this.
IMPORTANCE_FACTOR = 1.0

# This is another hyperparameter of the explanation only train step. Usually should be between 1 and 10
IMPORTANCE_MULTIPLIER = 10

# This is the number of explanation channels that are generated by the model. This is also the number of
# attention heads used in the graph convolutional layers of the network. So to get a "wider" network, this
# parameter can also be increased. However, note that the value can only be != 2 if importance factor is
# set to exactly 0.0!
IMPORTANCE_CHANNELS = 2

# The coefficient value of the explanation sparsity regularization that is applied to the network. Higher
# values should lead to sparser explanations.
SPARSITY_FACTOR = 0.1

# We need to supply the range of possible target values and a reference target value a priori to the
# network for the regression case. The regression limits should be as complete as possible. The reference
# does not have to be in the middle, changing it will influence the development of the explanations quite
# a bit.
REGRESSION_LIMITS = [-16, +2]
REGRESSION_REFERENCE = -2

# At the tail end of the network there is a MLP, which does the final prediction. This list defines the
# depth of that MLP. Each element adds a layer and the values themselves dictate the hidden values.
# In the regression case a final entry with "1" hidden layer as the output layer will implicitly be
# added.
FINAL_UNITS = [30, 10]

# This dropout is applied after EVERY layer in the final MLP. Using this should not be necessary
FINAL_DROPOUT = 0.0

# == TRAINING PARAMETERS ==
DEVICE = 'gpu:0'
LOSS = ks.losses.MeanSquaredError()
LEARNING_RATE = 0.001
BATCH_SIZE = 256
EPOCHS = 200

# == EVALUATION PARAMETERS ==
LOG_STEP = 20
METRIC_KEY = 'mean_squared_error'
CHANNEL_NAMES = {
    0: 'Negative',
    1: 'Positive'
}

DEBUG = True
with Skippable(), (e := Experiment(base_path=os.getcwd(), namespace='smiles_example', glob=globals())):

    # -- CONVERTING TO "EYE TRACKING DATASET"
    # First of all we convert the dataset into the "eye tracking" format. In this format, the dataset
    # is represented as a folder where each sample is associated with 2 files:
    # - One PNG file which visualizes that element
    # - One JSON file, which contains the graph representation of the molecule and other metadata
    folder_path = os.path.dirname(CSV_PATH)
    dataset_path = os.path.join(folder_path, 'eye_tracking_dataset')

    # We only do the conversion though if it does not already exists
    if not os.path.exists(dataset_path):
        e.info('starting to process dataset...')
        e.info('reading csv file...')
        molecule_infos: Dict[str, Dict[str, Any]] = {}
        with open(CSV_PATH, mode='r') as csv_file:
            reader = csv.DictReader(csv_file, delimiter=',', quotechar='"')
            for i, row in enumerate(reader):
                smile = row[SMILES_COLUMN]
                value = float(row[TARGET_COLUMN])
                element_id = str(i)

                molecule_info = {
                    'id': element_id,
                    'name': element_id,
                    'value': value,
                    'smiles': smile,
                    'input_type': 'regression',
                }
                molecule_infos[element_id] = molecule_info
                if i % LOG_STEP == 0:
                    e.info(f' * ({i:03d}) - smile: {smile}')

        e.info(f'creating eye tracking dataset in: {dataset_path}')
        os.mkdir(dataset_path)
        create_molecule_eye_tracking_dataset(
            molecule_infos=molecule_infos,
            dest_path=dataset_path,
            logger=e.logger,
            log_step=LOG_STEP,
            # This dictionary determines what kinds of chemical properties are going to be added as features
            # of the resulting dataset. There are two kinds of features: Ones associated with the
            # nodes (atoms) of the graph and the ones associated with the edges (bonds).
            # This dictionary will be passed almost exactly as it is to the "kgcnn.moleculenet.MoleculeNet"
            # class which does the actual interaction with RDKit, so look into that for more details about
            # what kinds of options are available.
            set_attributes_kwargs={
                'nodes': ['Symbol', 'TotalDegree', 'FormalCharge', 'NumRadicalElectrons', 'Hybridization',
                          'IsAromatic', 'IsInRing', 'TotalNumHs', 'CIPCode', 'ChiralityPossible',
                          'ChiralTag'],
                'encoder_nodes': {
                    'Symbol': OneHotEncoder(
                        ['B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br', 'Te', 'I', 'At'],
                        dtype="str"
                    ),
                    'Hybridization': OneHotEncoder([2, 3, 4, 5, 6]),
                    'TotalDegree': OneHotEncoder([0, 1, 2, 3, 4, 5], add_unknown=False),
                    'TotalNumHs': OneHotEncoder([0, 1, 2, 3, 4], add_unknown=False),
                    'CIPCode': OneHotEncoder(['R', 'S'], add_unknown=False, dtype='str'),
                    'ChiralityPossible': OneHotEncoder(["1"], add_unknown=False, dtype='str'),
                },
                'edges': ['BondType', 'IsAromatic', 'IsConjugated', 'IsInRing', 'Stereo'],
                'encoder_edges': {
                    'BondType': OneHotEncoder([1, 2, 3, 12], add_unknown=False),
                    'Stereo': OneHotEncoder([0, 1, 2, 3], add_unknown=False)
                },
            }
        )
        e.info(f'created eye tracking dataset with {len(molecule_infos)} elements!')

    # -- LOADING DATASET
    # Now we need to load the dataset and do some pre-processing so that we can use it for model training
    e.info('loading dataset...')
    eye_tracking_dataset = load_eye_tracking_dataset(dataset_path)
    e.info(f'loaded dataset with {len(eye_tracking_dataset)} elements')

    # the "eye_tracking_dataset" consists of dicts, which hold the information about the filepaths for the
    # image file, metadata file, the metadata dict itself, the graph dict. But for the training we only
    # need the graph dict, which is why we extract those here.
    dataset = []
    for data in eye_tracking_dataset:
        g = data['metadata']['graph']

        # We need to fill this field with the target value! In the future for multiple regression targets
        # this may also be a numpy array / vector. The important thing is that the output shape of the
        # network matches the shape of this!
        g['graph_labels'] = np.array(data['metadata']['value'])

        dataset.append(g)

    dataset_indices = list(range(len(dataset)))
    dataset_length = len(dataset)
    train_indices = random.sample(dataset_indices, k=int(TRAIN_SPLIT * len(dataset_indices)))
    test_indices = [index for index in dataset_indices if index not in train_indices]
    e['train_indices'] = train_indices
    e['test_indices'] = test_indices

    # This turns the list of graph dicts into the final form which we need for the training of the model:
    # keras RaggedTensors which contain all the graphs.
    x_train, y_train, x_test, y_test = process_graph_dataset(dataset, test_indices)

    # -- TRAINING THE MODEL
    model = MultiAttentionStudent(
        units=UNITS,
        dropout_rate=DROPOUT_RATE,
        use_bias=True,
        use_edge_features=True,
        importance_factor=IMPORTANCE_FACTOR,
        importance_multiplier=IMPORTANCE_MULTIPLIER,
        sparsity_factor=SPARSITY_FACTOR,
        regression_limits=REGRESSION_LIMITS,
        regression_reference=REGRESSION_REFERENCE,
        final_units=FINAL_UNITS,
        final_dropout_rate=FINAL_DROPOUT
    )

    model.compile(
        loss=[
            ks.losses.MeanSquaredError(),
            NoLoss(),
            NoLoss()
        ],
        loss_weights=[
            1,
            0,
            0
        ],
        metrics=[
            ks.metrics.MeanSquaredError(),
            ks.metrics.MeanAbsoluteError(),
        ],
        optimizer=ks.optimizers.Adam(learning_rate=LEARNING_RATE),
        run_eagerly=False
    )
    with tf.device('gpu:0'):
        history = model.fit(
            x_train,
            y_train,
            batch_size=BATCH_SIZE,
            epochs=EPOCHS,
            validation_data=(x_test, y_test),
            validation_freq=1,
            callbacks=LogProgressCallback(
                logger=e.logger,
                epoch_step=LOG_STEP,
                identifier=f'val_output_1_{METRIC_KEY}'
            ),
            verbose=0
        )

    e[f'history'] = history.history
    e[f'epochs'] = list(range(EPOCHS))
    param_count = model.count_params()
    e['param_count'] = param_count
    e.info(f'number of parameters: {param_count}')

    # -- EVALUATING TEST SET
    # Now we get to the reason why we converted the dataset into the "eye tracking" format first: This will
    # now simplify the visualization of the results a lot!
    # We now evaluate all the elements in the test set and visualize the results. We will do this by
    # creating one big PDF document which displays all the elements of the test set and then also displays
    # what kinds of explanations the model predicted for those.
    e.info('Evaluating test set and drawing examples...')
    pdf_path = os.path.join(e.path, f'visualization.pdf')
    ncols = 1
    nrows = IMPORTANCE_CHANNELS
    with PdfPages(pdf_path) as pdf:
        for c, index in enumerate(test_indices):

            data = eye_tracking_dataset[index]
            g = data['metadata']['graph']
            out_true = g['graph_labels']
            # First of all we need to pass the corresponding graph through the model to get the prediction
            # and the explanation tensors. ni = node_importances, ei = edge_importances
            out_pred, ni, ei = model.predict_single([
                g['node_attributes'],
                g['edge_attributes'],
                g['edge_indices']
            ])
            ni = np.array(ni)
            ei = np.array(ei)
            e[f'out/true/{index}'] = out_true
            e[f'out/pred/{index}'] = out_pred
            e[f'ni/{index}'] = ni
            e[f'ei/{index}'] = ei

            image = np.asarray(imread(data['image_path']))
            fig, rows = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8 * ncols, 8 * nrows), squeeze=False)
            fig.suptitle(f'Element {data["metadata"]["name"]}\n'
                         f'{data["metadata"]["smiles"]}\n'
                         f'True: {out_true:.2f} - Pred: {out_pred:.2f}')
            for row_index in range(nrows):
                column_index = 0

                ax = rows[row_index][column_index]
                ax.set_ylabel(f'Explanation Channel {row_index}:\n'
                              f'{CHANNEL_NAMES[row_index]}')
                ax.imshow(image, extent=(0, image.shape[0], 0, image.shape[1]))
                plot_node_importances(
                    g=g,
                    ax=ax,
                    vmax=np.max(ni),
                    node_importances=ni[:, row_index],
                    node_coordinates=np.array(g['node_coordinates'])
                )
                plot_edge_importances(
                    g=g,
                    ax=ax,
                    vmax=np.max(ei),
                    edge_importances=ei[:, row_index],
                    node_coordinates=np.array(g['node_coordinates'])
                )

            pdf.savefig(fig)
            plt.close(fig)

            if c % LOG_STEP == 0:
                e.info(f' * ({c}/{len(test_indices)})')
