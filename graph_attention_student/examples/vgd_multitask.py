"""
This example shows how to train a MEGAN model for multitask graph regression application based on an
existing "visual graph dataset"
"""
import os
import warnings
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import csv
import random
import typing as t

from pycomex.experiment import Experiment
from pycomex.util import Skippable

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as ks
from matplotlib.backends.backend_pdf import PdfPages
from kgcnn.data.moleculenet import OneHotEncoder
from kgcnn.data.utils import ragged_tensor_from_nested_numpy
from imageio.v2 import imread
from sklearn.metrics import r2_score
from visual_graph_datasets.util import get_dataset_path
from visual_graph_datasets.data import load_visual_graph_dataset

import graph_attention_student.typing as tc
from graph_attention_student.util import DATASETS_FOLDER
from graph_attention_student.data import create_molecule_eye_tracking_dataset
from graph_attention_student.data import load_eye_tracking_dataset
from graph_attention_student.data import load_eye_tracking_dataset_dict
from graph_attention_student.data import process_graph_dataset
from graph_attention_student.training import NoLoss, mse
from graph_attention_student.training import LogProgressCallback
from graph_attention_student.models import MultiAttentionStudent
from graph_attention_student.models import Megan
from graph_attention_student.visualization import plot_node_importances
from graph_attention_student.visualization import plot_edge_importances

np.set_printoptions(precision=3)

# == DATASET PARAMETERS ==

# The name of the visual graph dataset to use for this experiment.
VISUAL_GRAPH_DATASET_PATH = os.path.expanduser('~/.visual_graph_datasets/datasets/organic_solvents')
# The ratio of how many elements of the dataset are supposed to be used for the training dataset.
# The rest of them will be used for the test set.
TRAIN_RATIO = 0.8
# The number of target values for each graph in the dataset.
NUM_TARGETS = 3
# Whether or not the dataset already contains importance (explanation) ground truth annotations.
# most of the time this will most likely not be the case
HAS_IMPORTANCES = False

# == MODEL PARAMETERS ==

# This list defines how many graph convolutional layers to configure the network with. Every element adds
# one layer. The numbers themselves are the hidden units to be used for those layers.
UNITS = [30, 30, 30]

# The dropout rate which is applied after EVERY graph convolutional layer of the network. Especially for
# large networks (>20 hidden units and multiple importance channels, this dropout proves very useful)
DROPOUT_RATE = 0.0

# This is the weight of the additional explanation-only step which is being applied to the network.
# This explanation only step is important to develop actually interpretable explanations. Refer to the
# paper for more details about this.
IMPORTANCE_FACTOR = 0.0

# This is another hyperparameter of the explanation only train step. Usually should be between 1 and 10
IMPORTANCE_MULTIPLIER = 10

# This is the number of explanation channels that are generated by the model. This is also the number of
# attention heads used in the graph convolutional layers of the network. So to get a "wider" network, this
# parameter can also be increased. However, note that the value can only be != 2 if importance factor is
# set to exactly 0.0!
IMPORTANCE_CHANNELS = 3

# The coefficient value of the explanation sparsity regularization that is applied to the network. Higher
# values should lead to sparser explanations.
SPARSITY_FACTOR = 1.0

# We need to supply the range of possible target values and a reference target value a priori to the
# network for the regression case. The regression limits should be as complete as possible. The reference
# does not have to be in the middle, changing it will influence the development of the explanations quite
# a bit.
REGRESSION_LIMITS = [-6, +2]
REGRESSION_REFERENCE = 0

# At the tail end of the network there is a MLP, which does the final prediction. This list defines the
# depth of that MLP. Each element adds a layer and the values themselves dictate the hidden values.
# In the regression case a final entry with "1" hidden layer as the output layer will implicitly be
# added.
FINAL_UNITS = [30, NUM_TARGETS]

# This dropout is applied after EVERY layer in the final MLP. Using this should not be necessary
FINAL_DROPOUT = 0.0

# == TRAINING PARAMETERS ==
DEVICE = 'gpu:0'
LEARNING_RATE = 0.001
BATCH_SIZE = 64
EPOCHS = 100

# == EVALUATION PARAMETERS ==
LOG_STEP = 5
LOG_STEP_EVAL = 1000
METRIC_KEY = 'mean_squared_error'

# == EXPERIMENT PARAMETERS ==
DEBUG = True
BASE_PATH = os.getcwd()
NAMESPACE = 'vgd_multitask'
with Skippable(), (e := Experiment(base_path=BASE_PATH, namespace=NAMESPACE, glob=globals())):
    e.info('starting experiment for VGD multitask example...')

    e.info('loading dataset...')
    # TODO: Download if it does not exist.
    name_data_map, index_data_map = load_visual_graph_dataset(
        path=VISUAL_GRAPH_DATASET_PATH,
        logger=e.logger,
        log_step=LOG_STEP_EVAL,
        metadata_contains_index=True
    )
    dataset_indices = list(index_data_map.keys())
    dataset: t.List[tc.GraphDict] = []
    for index, data in index_data_map.items():
        g = data['metadata']['graph']
        # g['graph_labels'] = [v if v is not None else 0 for v in g['graph_labels']]
        if not HAS_IMPORTANCES:
            g['node_importances'] = np.zeros(shape=(len(g['node_indices']), IMPORTANCE_CHANNELS))
            g['edge_importances'] = np.zeros(shape=(len(g['edge_indices']), IMPORTANCE_CHANNELS))

        dataset.append(g)

    train_indices = random.sample(dataset_indices, k=int(TRAIN_RATIO * len(dataset_indices)))
    test_indices = [index for index in dataset_indices if index not in train_indices]
    e['train_indices'] = train_indices
    e['test_indices'] = test_indices

    # This turns the list of graph dicts into the final form which we need for the training of the model:
    # keras RaggedTensors which contain all the graphs.
    x_train, y_train, x_test, y_test = process_graph_dataset(
        dataset,
        train_indices=train_indices,
        test_indices=test_indices
    )

    # -- TRAINING THE MODEL
    model = Megan(
        units=UNITS,
        final_units=FINAL_UNITS,
        use_bias=True,
        importance_channels=IMPORTANCE_CHANNELS,
        importance_factor=IMPORTANCE_FACTOR,
        sparsity_factor=SPARSITY_FACTOR,
        regression_limits=REGRESSION_LIMITS,
        regression_reference=REGRESSION_REFERENCE,
        dropout_rate=DROPOUT_RATE
    )

    model.compile(
        loss=[
            mse,
            NoLoss(),
            NoLoss()
        ],
        loss_weights=[
            1,
            0,
            0
        ],
        metrics=[
            ks.metrics.MeanSquaredError(),
            ks.metrics.MeanAbsoluteError(),
        ],
        optimizer=ks.optimizers.Adam(learning_rate=LEARNING_RATE),
        run_eagerly=False
    )
    with tf.device('gpu:0'):
        e.info('starting model training...')
        history = model.fit(
            x_train,
            y_train,
            batch_size=BATCH_SIZE,
            epochs=EPOCHS,
            validation_data=(x_test, y_test),
            validation_freq=1,
            callbacks=LogProgressCallback(
                logger=e.logger,
                epoch_step=LOG_STEP,
                identifier=f'val_output_1_{METRIC_KEY}'
            ),
            verbose=0
        )

