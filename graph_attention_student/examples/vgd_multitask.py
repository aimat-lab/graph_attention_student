"""
This example shows how to train a MEGAN model for multitask graph regression application based on an
existing "visual graph dataset"
"""
import os
import warnings
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import csv
import random
import typing as t
from itertools import product

from pycomex.experiment import Experiment
from pycomex.util import Skippable

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as ks
from matplotlib.backends.backend_pdf import PdfPages
from kgcnn.data.moleculenet import OneHotEncoder
from kgcnn.data.utils import ragged_tensor_from_nested_numpy
from imageio.v2 import imread
from sklearn.metrics import r2_score
from visual_graph_datasets.util import get_dataset_path
from visual_graph_datasets.data import load_visual_graph_dataset
from visual_graph_datasets.visualization.importances import create_importances_pdf

import graph_attention_student.typing as tc
from graph_attention_student.util import DATASETS_FOLDER
from graph_attention_student.data import create_molecule_eye_tracking_dataset
from graph_attention_student.data import load_eye_tracking_dataset
from graph_attention_student.data import load_eye_tracking_dataset_dict
from graph_attention_student.data import process_graph_dataset
from graph_attention_student.training import NoLoss, mse
from graph_attention_student.training import LogProgressCallback
from graph_attention_student.models import MultiAttentionStudent
from graph_attention_student.models import Megan
from graph_attention_student.visualization import plot_node_importances
from graph_attention_student.visualization import plot_edge_importances

np.set_printoptions(precision=2)


SHORT_DESCRIPTION = (
    'This example shows how to train a MEGAN model for multitask graph regression application based on an existing '
    'visual graph dataset.'
)

# == DATASET PARAMETERS ==

# The name of the visual graph dataset to use for this experiment.
VISUAL_GRAPH_DATASET_PATH = os.path.expanduser('~/.visual_graph_datasets/datasets/organic_solvents')
# The ratio of how many elements of the dataset are supposed to be used for the training dataset.
# The rest of them will be used for the test set.
TRAIN_RATIO = 0.8
# The number of target values for each graph in the dataset.
NUM_TARGETS = 4
# Whether the dataset already contains importance (explanation) ground truth annotations.
# most of the time this will most likely not be the case
HAS_IMPORTANCES: bool = False
# IF the dataset includes global graph attributes and if they are supposed to be used in the training process, this
# flag has to be set to True.
HAS_GRAPH_ATTRIBUTES: bool = True
# The ratio of the test set to be used as examples for the visualization of the explanations
EXAMPLES_RATIO: float = 0.2
# The string names of the target values in the order in which they appear in the dataset as well
# which will be used in the labels for the result visualizations
TARGET_NAMES = [
    'water',
    'benzene',
    'acetone',
    'ethanol',
]

# == MODEL PARAMETERS ==

# This list defines how many graph convolutional layers to configure the network with. Every element adds
# one layer. The numbers themselves are the hidden units to be used for those layers.
UNITS = [20, 20, 20, 20]

# The dropout rate which is applied after EVERY graph convolutional layer of the network. Especially for
# large networks (>20 hidden units and multiple importance channels, this dropout proves very useful)
DROPOUT_RATE = 0.3

# This is the weight of the additional explanation-only step which is being applied to the network.
# This explanation only step is important to develop actually interpretable explanations. Refer to the
# paper for more details about this.
IMPORTANCE_FACTOR = 1.0

# This is another hyperparameter of the explanation only train step. Usually should be between 1 and 10
IMPORTANCE_MULTIPLIER = 5

# This is the number of explanation channels that are generated by the model. This is also the number of
# attention heads used in the graph convolutional layers of the network. So to get a "wider" network, this
# parameter can also be increased. However, note that the value can only be != 2 if importance factor is
# set to exactly 0.0!
IMPORTANCE_CHANNELS = 8

# The coefficient value of the explanation sparsity regularization that is applied to the network. Higher
# values should lead to sparser explanations.
SPARSITY_FACTOR = 5.0

# We need to supply the range of possible target values and a reference target value a priori to the
# network for the regression case. The regression limits should be as complete as possible. The reference
# does not have to be in the middle, changing it will influence the development of the explanations quite
# a bit.
REGRESSION_REFERENCE = [
    -3,  # water
    -1,  # benzene
    -1,  # acetone
    -1,  # ethanol
]
REGRESSION_LIMITS = [
    [-10, +2],
    [-4, +1],
    [-4, +1],
    [-4, +1],
]

# At the tail end of the network there is a MLP, which does the final prediction. This list defines the
# depth of that MLP. Each element adds a layer and the values themselves dictate the hidden values.
# In the regression case a final entry with "1" hidden layer as the output layer will implicitly be
# added.
FINAL_UNITS = [50, 20, NUM_TARGETS]

# This dropout is applied after EVERY layer in the final MLP. Using this should not be necessary
FINAL_DROPOUT = 0.0

# == TRAINING PARAMETERS ==
DEVICE = 'gpu:0'
LEARNING_RATE = 0.001
BATCH_SIZE = 64
EPOCHS = 200

# == EVALUATION PARAMETERS ==
LOG_STEP = 5
LOG_STEP_EVAL = 1000
METRIC_KEY = 'mean_squared_error'

# == EXPERIMENT PARAMETERS ==
DEBUG = True
BASE_PATH = os.getcwd()
NAMESPACE = 'results/vgd_multitask'
with Skippable(), (e := Experiment(base_path=BASE_PATH, namespace=NAMESPACE, glob=globals())):
    e.info('starting experiment for VGD multitask example...')

    e.info('loading dataset...')
    name_data_map, index_data_map = load_visual_graph_dataset(
        path=VISUAL_GRAPH_DATASET_PATH,
        logger=e.logger,
        log_step=LOG_STEP_EVAL,
        metadata_contains_index=True
    )
    dataset_indices = list(sorted(index_data_map.keys()))
    dataset: t.List[tc.GraphDict] = []
    for index in dataset_indices:
        data = index_data_map[index]
        g = data['metadata']['graph']
        # g['graph_labels'] = [v if v is not None else 0 for v in g['graph_labels']]
        if not HAS_IMPORTANCES:
            g['node_importances'] = np.zeros(shape=(len(g['node_indices']), IMPORTANCE_CHANNELS))
            g['edge_importances'] = np.zeros(shape=(len(g['edge_indices']), IMPORTANCE_CHANNELS))

        dataset.append(g)

    train_indices = random.sample(dataset_indices, k=int(TRAIN_RATIO * len(dataset_indices)))
    test_indices = [index for index in dataset_indices if index not in train_indices]
    e['train_indices'] = train_indices
    e['test_indices'] = test_indices

    # This turns the list of graph dicts into the final form which we need for the training of the model:
    # keras RaggedTensors which contain all the graphs.
    x_train, y_train, x_test, y_test = process_graph_dataset(
        dataset,
        train_indices=train_indices,
        test_indices=test_indices,
        use_graph_attributes=HAS_GRAPH_ATTRIBUTES,
    )

    # -- TRAINING THE MODEL
    model = Megan(
        units=UNITS,
        final_units=FINAL_UNITS,
        use_bias=True,
        importance_channels=IMPORTANCE_CHANNELS,
        importance_factor=IMPORTANCE_FACTOR,
        sparsity_factor=SPARSITY_FACTOR,
        regression_limits=REGRESSION_LIMITS,
        regression_reference=REGRESSION_REFERENCE,
        dropout_rate=DROPOUT_RATE,
        use_graph_attributes=HAS_GRAPH_ATTRIBUTES,
    )

    model.compile(
        loss=[
            mse,
            NoLoss(),
            NoLoss()
        ],
        loss_weights=[
            1,
            0,
            0
        ],
        metrics=[
            mse,
            ks.metrics.MeanAbsoluteError(),
        ],
        optimizer=ks.optimizers.Adam(learning_rate=LEARNING_RATE),
        run_eagerly=False
    )
    with tf.device('gpu:0'):
        e.info('starting model training...')
        history = model.fit(
            x_train,
            y_train,
            batch_size=BATCH_SIZE,
            epochs=EPOCHS,
            validation_data=(x_test, y_test),
            validation_freq=1,
            callbacks=LogProgressCallback(
                logger=e.logger,
                epoch_step=LOG_STEP,
                identifier=f'val_output_1_{METRIC_KEY}'
            ),
            verbose=0
        )
    # -- EVALUATION OF TEST SET --
    e.info('evaluating test set...')

    # first of all we need to pipe the entire test set through the model to get the target value predictions as well
    # as the predicted node and edge importance explanations
    out_pred, ni_pred, ei_pred = model(x_test)

    example_indices = random.sample(test_indices, k=int(EXAMPLES_RATIO * len(test_indices)))
    x_example, y_example, _, _ = process_graph_dataset(
        dataset=dataset,
        train_indices=example_indices,
        test_indices=example_indices,
        use_graph_attributes=HAS_GRAPH_ATTRIBUTES,
    )
    out_example, ni_example, ei_example = model(x_example)
    out_example = out_example.numpy()
    ni_example = ni_example.numpy()
    ei_example = ei_example.numpy()

    # -- VISUALIZATION OF RESULTS --
    e.info(f'visualizing {len(example_indices)} example explanations...')
    pdf_path = os.path.join(e.path, 'examples.pdf')
    graph_list = [index_data_map[i]['metadata']['graph'] for i in example_indices]
    create_importances_pdf(
        graph_list=graph_list,
        image_path_list=[index_data_map[i]['image_path'] for i in example_indices],
        node_positions_list=[index_data_map[i]['metadata']['graph']['node_positions'] for i in example_indices],
        importances_map={
            'model': (ni_example, ei_example)
        },
        labels_list=[f'true {index_data_map[i]["metadata"]["target"]} - pred {out_example[c]}'
                     for c, i in enumerate(example_indices)],
        output_path=pdf_path,
        importance_channel_labels=[f'{name} {direction}'
                                   for name in TARGET_NAMES
                                   for direction in ['negative', 'positive']]
    )

